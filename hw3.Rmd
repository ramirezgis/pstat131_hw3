---
title: "hw3"
output: html_document
date: "2022-10-18"
---

```{r setup, include=FALSE}
library(klaR)
library(discrim)
library(tidymodels)
library(tidyverse)
library(corrplot)
tidymodels_prefer()
titanic <- read.csv("data/titanic.csv") %>% 
  mutate(survived = factor(survived))
titanic$pclass <- as.factor(titanic$pclass)
titanic
set.seed(1234)
```

Question 1:

```{r }
titanic_split <- initial_split(titanic, 
                               prop = 0.80, 
                               strata = survived)
titanic_split

train <- training(titanic_split)
test <- testing(titanic_split)
```

The training data set has 712 observations and the testing data set has 179 observations. This makes sense as the total observations the titanic data set has is 891, which is 179 + 712.

There seems to be a lot of missing data for the variable cabin with 552 missing values; age has 136 missing values and embarked variable has 2 missing values. 

It is good to use stratified sampling for this data as we want to see the characteristics for each of the survivor and non-survivor groups and see why the survivor group is more likely to survive. 

Question 2:

```{r}
dist_surv <- table(train$survived)
dist_surv

barplot((table(train$survived)), col = "pink")
```

There seems to be more values for not surviving (439 observations) than surviving (273 observations).

Question 3:
```{r}
#install.packages("corrr")
library(corrr)
library(ggplot2)

train %>%
  select_if(is.numeric) %>%
  cor(use = "complete.obs") %>%
  corrplot(type = "lower", diag = FALSE, method = "color")
```

I see that age and sib_sp are negatively correlated, and age and parch are also negatively correlated but not as high as age and sib_sp. Meanwhile sib_sp and parch are almost highly postively correlated. The other variables have little to no correlation. 

Question 4:
```{r}
titanic_recipe <-
  recipe(survived ~ pclass+sex+age+sib_sp+parch+fare, data = train) %>%
  step_impute_linear(age,
                     impute_with = imp_vars(sib_sp, parch, fare)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~starts_with("sex"):fare + age:fare)
titanic_recipe
#summary(train)
```

Question 5
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, train)

log_fit %>% 
  tidy()
```

Question 6
```{r }
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, train)

lda_fit
```

Question 7
```{r}
qda_mod <- discrim_quad() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, train)

log_fit %>% 
  tidy()
```


Question 8
```{r}
nb_mod <- naive_Bayes() %>% 
  set_engine("klaR") %>% 
  set_mode("classification") %>%
  set_args(usekernel = FALSE)

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, train)

nb_fit

```

Question 9
```{r, warning=FALSE}
log_pred <- predict(log_fit, new_data = train, 
                      type = "class") %>%
  bind_cols(train %>% select(survived))

log_acc <- log_pred %>%
  accuracy(truth = survived, estimate = .pred_class)

lda_pred <- predict(lda_fit, new_data = train,
                    type = "class") %>%
  bind_cols(train %>% select(survived))

lda_acc <- lda_pred %>%
  accuracy(truth = survived, estimate = .pred_class)

qda_pred <- predict(qda_fit, new_data = train, 
                    type = "class") %>%
  bind_cols(train %>% select(survived))

qda_acc <- qda_pred %>%
  accuracy(truth = survived, estimate = .pred_class)

nb_pred <- predict(nb_fit, new_data = train, 
                   type = "class") %>%
  bind_cols(train %>% select(survived))

nb_acc <- nb_pred %>%
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log_acc$.estimate, lda_acc$.estimate, 
                qda_acc$.estimate, nb_acc$.estimate)
models <- c("Logistic Regression", "LDA", "QDA", 
            "Naive Bayes")
results <- tibble(accuracies = accuracies, models = models)
results %>%
  arrange(-accuracies)
```

log_acc = 0.81
lda_acc = 0.79
qda_acc = 0.80
nb_acc = 0.77

The log fit had the highest accuracy on the training data.

Question 10
```{r}
predict(log_fit, new_data = test, type = "prob")
augment(log_fit, new_data = test) %>%
  conf_mat(truth = survived, estimate = .pred_class)

multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(log_fit, new_data = test) %>%
  multi_metric(truth = survived, estimate = .pred_class)

augment(log_fit, new_data = test) %>%
  roc_curve(survived, .pred_No) %>%
  autoplot()

augment(log_fit, new_data = test) %>%
  roc_auc(survived, .pred_No) 
```

This model performed well (not super accurate but it is above 50%). The training predicted 81% accuracy, but the testing got 78% accuracy, which is still pretty good. They do differ a bit, but it's a not a big difference and the small difference may be due to the fact that it was different subjects that were tested. 
